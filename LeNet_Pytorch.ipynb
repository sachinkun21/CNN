{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet:Pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8+LXaade7onhUvX14NyYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinkun21/CNN/blob/master/LeNet_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGoAGyqeGBRh",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "LeNet-5, from the paper Gradient-Based Learning Applied to Document Recognition, is a very efficient convolutional neural network for handwritten character recognition.\n",
        "\n",
        "\n",
        "<a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\">Paper: <u>Gradient-Based Learning Applied to Document Recognition</u></a>\n",
        "\n",
        "**Authors**: Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner\n",
        "\n",
        "**Published in**: Proceedings of the IEEE (1998)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM3OGarJFA8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntYG91YlFVCM",
        "colab_type": "code",
        "outputId": "76f62a15-b922-4601-c657-78306fb545ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,5)\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(16*4*4,120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self,x):\n",
        "    size = x.size()[1:] \n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "        num_features *= s\n",
        "    return num_features\n",
        "\n",
        "lenet = LeNet()\n",
        "print(lenet)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tN5LssxKvu8",
        "colab_type": "code",
        "outputId": "e225276d-adeb-4e26-8d93-dc3864067c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "params = list(lenet.parameters())\n",
        "print(len(params))\n",
        "print(params[2].size())  # conv1's .weight"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([16, 6, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gacYoUYK4JV",
        "colab_type": "code",
        "outputId": "55468802-044d-4a1a-8829-d4b09c69a66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "inp = torch.randn(1, 1, 28,28)\n",
        "out = lenet(inp)\n",
        "print(out)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0430,  0.0578, -0.0827, -0.0121, -0.0453, -0.0078,  0.0431,  0.1317,\n",
            "          0.0858, -0.0289]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HyJbLWmSbIA",
        "colab_type": "code",
        "outputId": "3a00a8b0-25d8-4b58-c310-f7512c5a89b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inp.dtype"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqSXt-Y4LHcM",
        "colab_type": "code",
        "outputId": "d04a36a4-6ee9-4e17-d4ee-128cacbc36a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Loading the dataset and perform splitting\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "# Peforming reshaping operation\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
        "\n",
        "# Normalization\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# One Hot Encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "# Building the Model Architecture"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es7EqzMZLpyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(lenet.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "\n",
        "input = torch.from_numpy(x_train).type(torch.float32)\n",
        "target = torch.from_numpy(y_train).type(torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK5AfSjkTxqS",
        "colab_type": "code",
        "outputId": "64bec51a-bb25-4ec6-b201-bb8edcf81802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input.shape,target.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 1, 28, 28]), torch.Size([60000, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWt9Nk_pUTg_",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyZGOWQTaN52",
        "colab_type": "code",
        "outputId": "374d92da-af9e-4cb6-c355-69d689f9bd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 1, 28, 28), (60000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1eIN1fTtcdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.from_numpy(x_train).type(torch.float32)\n",
        "y_train = torch.from_numpy(y_train).type(torch.float32)\n",
        "\n",
        "x_test = torch.from_numpy(x_test).type(torch.float32)\n",
        "y_test = torch.from_numpy(y_test).type(torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTRgL6jBglaN",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Training: Upgrading gradients for each data-point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "149gymchaN2w",
        "colab_type": "code",
        "outputId": "bed83635-b5ed-4650-fad6-a3869f14736d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(lenet.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "for epoch in range(1,5):\n",
        "  start = time.time()\n",
        "  train_loss, valid_loss = [], []\n",
        "\n",
        "  # Training \n",
        "  lenet.train()\n",
        "  for data , target in zip(x_train,y_train):\n",
        "      \n",
        "      data = data.view(-1,1,28,28)\n",
        "      target = target.view(-1,10)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 1. Forward Propagation\n",
        "      output = lenet(data)\n",
        "\n",
        "      # 2. Loss Calculation\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      # 3. Backward propagation\n",
        "      loss.backward()\n",
        "\n",
        "      # 4. Weight Optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "  stop = time.time()\n",
        "  # Evaluation \n",
        "  lenet.eval()\n",
        "  for data, target in zip(x_test,y_test):\n",
        "    data = data.view(-1,1,28,28)\n",
        "    target = target.view(-1,10)\n",
        "\n",
        "    output = lenet(data)\n",
        "    loss = criterion(output , target)\n",
        "    valid_loss.append(loss.item())\n",
        "  print(\"Epoch:\" , epoch, \"\\tTraining Loss:\", np.mean(train_loss) , \"\\tValidation Loss:\", np.mean(valid_loss), \"\\tTime Taken:\" ,stop-start)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7229d19b0516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNvP7iDyggkM",
        "colab_type": "text"
      },
      "source": [
        "### Mini Batch Training: Upgrading Gradients for Each set(Batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlSmATUacBd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "19e547f1-e581-4a74-d72a-85de1d2e0034"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(lenet.parameters(), lr=0.01)\n",
        "batch_size = 256\n",
        "step_size = len(x_train)//128\n",
        "\n",
        "\n",
        "# To creates batches of Data\n",
        "def return_mini_batch( x_train,y_train,batch_size = 128):\n",
        "  indexes = np.random.randint(0,len(x_train),batch_size)\n",
        "  return  x_train[indexes],y_train[indexes]\n",
        "\n",
        "# x_train_b ,y_train_b = return_mini_batch(x_train,y_train)\n",
        "# x_train_b.shape\n",
        "\n",
        "# TRAINING: MGD\n",
        "for epoch in range(1,5):\n",
        "  start = time.time()\n",
        "  train_loss, valid_loss = [], []\n",
        "\n",
        "  # Training \n",
        "  lenet.train()\n",
        "  for step in range(step_size):\n",
        "      data,target = return_mini_batch(x_train,y_train,step_size)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 1. Forward Propagation\n",
        "      output = lenet(data)\n",
        "\n",
        "      # 2. Loss Calculation\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      # 3. Backward propagation\n",
        "      loss.backward()\n",
        "\n",
        "      # 4. Weight Optimization\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss.append(loss.item())\n",
        "  stop = time.time()\n",
        "  \n",
        "  # Evaluation \n",
        "  lenet.eval()\n",
        "  output = lenet(x_test)\n",
        "  loss = criterion(output , y_test)\n",
        "  valid_loss.append(loss.item())\n",
        "\n",
        "  print(\"Epoch:\" , epoch, \"\\tTraining Loss:\", np.mean(train_loss) , \"\\tValidation Loss:\", np.mean(valid_loss), \"\\tTime Taken:\" ,stop-start)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.085113576032285 \tValidation Loss: 0.08393959701061249 \tTime Taken: 38.121838331222534\n",
            "Epoch: 2 \tTraining Loss: 0.08279042894768919 \tValidation Loss: 0.08100586384534836 \tTime Taken: 37.94103026390076\n",
            "Epoch: 3 \tTraining Loss: 0.0794776533690528 \tValidation Loss: 0.07720340043306351 \tTime Taken: 38.080740451812744\n",
            "Epoch: 4 \tTraining Loss: 0.07531313927700886 \tValidation Loss: 0.07262440025806427 \tTime Taken: 38.30573225021362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rwkzV_Tcr7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# indexes = np.random.randint(0,len(x_train),128)\n",
        "# print(indexes)\n",
        "# minibatch = x_train[indexes],y_train[indexes]\n",
        "# len(minibatch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdFX4nOojotA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Loading the MNIST Dataset and applying transform function\n",
        "mnist = MNIST(\"data\" , download = True , train = True)\n",
        "mnist[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd-2fThwS1kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}